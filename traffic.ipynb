{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6f6a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cee3e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"C:\\\\Users\\\\sanga\\\\Downloads\\\\archive (5)\\\\Indian-Traffic Sign-Dataset\\\\Images\"\n",
    "IMG_SIZE = 64\n",
    "NUM_CLASSES = 59\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load images in correct numerical order\n",
    "for folder_name in sorted(os.listdir(DATASET_PATH), key=lambda x: int(x)):\n",
    "    folder_path = os.path.join(DATASET_PATH, folder_name)\n",
    "    label = int(folder_name)\n",
    "    \n",
    "    for img_file in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "X = np.array(images) / 255.0  # Normalize\n",
    "y = to_categorical(np.array(labels), num_classes=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3f7e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "371d48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd414d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be4d97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=False\n",
    ")\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20290370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 38ms/step - accuracy: 0.1950 - loss: 3.2360 - val_accuracy: 0.6200 - val_loss: 1.3284\n",
      "Epoch 2/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - accuracy: 0.5838 - loss: 1.3976 - val_accuracy: 0.7449 - val_loss: 0.8458\n",
      "Epoch 3/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.7272 - loss: 0.8657 - val_accuracy: 0.7803 - val_loss: 0.6720\n",
      "Epoch 4/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.7820 - loss: 0.6608 - val_accuracy: 0.8018 - val_loss: 0.6059\n",
      "Epoch 5/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.8176 - loss: 0.5228 - val_accuracy: 0.8157 - val_loss: 0.5428\n",
      "Epoch 6/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.8316 - loss: 0.4725 - val_accuracy: 0.8218 - val_loss: 0.5389\n",
      "Epoch 7/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - accuracy: 0.8494 - loss: 0.4256 - val_accuracy: 0.8265 - val_loss: 0.5309\n",
      "Epoch 8/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.8723 - loss: 0.3521 - val_accuracy: 0.8322 - val_loss: 0.5012\n",
      "Epoch 9/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.8772 - loss: 0.3494 - val_accuracy: 0.8447 - val_loss: 0.5098\n",
      "Epoch 10/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.8907 - loss: 0.3072 - val_accuracy: 0.8415 - val_loss: 0.5145\n",
      "Epoch 11/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.9003 - loss: 0.2878 - val_accuracy: 0.8361 - val_loss: 0.5314\n",
      "Epoch 12/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.8997 - loss: 0.2772 - val_accuracy: 0.8358 - val_loss: 0.5831\n",
      "Epoch 13/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.9098 - loss: 0.2657 - val_accuracy: 0.8465 - val_loss: 0.5030\n",
      "Epoch 14/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.9129 - loss: 0.2447 - val_accuracy: 0.8436 - val_loss: 0.5384\n",
      "Epoch 15/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.9171 - loss: 0.2360 - val_accuracy: 0.8494 - val_loss: 0.5250\n",
      "Epoch 16/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.9094 - loss: 0.2604 - val_accuracy: 0.8447 - val_loss: 0.5471\n",
      "Epoch 17/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 149ms/step - accuracy: 0.9244 - loss: 0.2286 - val_accuracy: 0.8494 - val_loss: 0.5257\n",
      "Epoch 18/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 34ms/step - accuracy: 0.9282 - loss: 0.2149 - val_accuracy: 0.8340 - val_loss: 0.5734\n",
      "Epoch 19/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 34ms/step - accuracy: 0.9281 - loss: 0.2069 - val_accuracy: 0.8508 - val_loss: 0.5662\n",
      "Epoch 20/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - accuracy: 0.9280 - loss: 0.2085 - val_accuracy: 0.8526 - val_loss: 0.5554\n",
      "Epoch 21/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 36ms/step - accuracy: 0.9313 - loss: 0.2001 - val_accuracy: 0.8519 - val_loss: 0.6046\n",
      "Epoch 22/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 33ms/step - accuracy: 0.9328 - loss: 0.1996 - val_accuracy: 0.8490 - val_loss: 0.6378\n",
      "Epoch 23/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.9344 - loss: 0.1984 - val_accuracy: 0.8458 - val_loss: 0.6133\n",
      "Epoch 24/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.9356 - loss: 0.1965 - val_accuracy: 0.8558 - val_loss: 0.5367\n",
      "Epoch 25/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 35ms/step - accuracy: 0.9340 - loss: 0.1922 - val_accuracy: 0.8436 - val_loss: 0.5976\n",
      "Epoch 26/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 37ms/step - accuracy: 0.9387 - loss: 0.1823 - val_accuracy: 0.8519 - val_loss: 0.5639\n",
      "Epoch 27/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 41ms/step - accuracy: 0.9395 - loss: 0.1751 - val_accuracy: 0.8519 - val_loss: 0.5856\n",
      "Epoch 28/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 39ms/step - accuracy: 0.9465 - loss: 0.1736 - val_accuracy: 0.8476 - val_loss: 0.5877\n",
      "Epoch 29/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 38ms/step - accuracy: 0.9414 - loss: 0.1772 - val_accuracy: 0.8551 - val_loss: 0.5898\n",
      "Epoch 30/30\n",
      "\u001b[1m350/350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 41ms/step - accuracy: 0.9441 - loss: 0.1669 - val_accuracy: 0.8576 - val_loss: 0.5858\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f70862b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"traffic_sign_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee859689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: Telephone\n",
      "Detected: Restaurant\n",
      "Detected: Give way\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "\n",
    "model = load_model(\"traffic_sign_model.h5\")\n",
    "IMG_SIZE = 64\n",
    "\n",
    "\n",
    "class_labels = [\n",
    "    \"Give way\",                          # 0\n",
    "    \"No entry\",                          # 1\n",
    "    \"One-way traffic\",                   # 2\n",
    "    \"One-way traffic\",                   # 3 (same as 2)\n",
    "    \"No vehicles in both directions\",    # 4\n",
    "    \"No entry for cycles\",               # 5\n",
    "    \"No entry for goods vehicles\",       # 6\n",
    "    \"No entry for pedestrians\",          # 7\n",
    "    \"No entry for bullock carts\",        # 8\n",
    "    \"No entry for hand carts\",           # 9\n",
    "    \"No entry for motor vehicles\",       #10\n",
    "    \"Height limit\",                      #11\n",
    "    \"Weight limit\",                      #12\n",
    "    \"Axle weight limit\",                 #13\n",
    "    \"Length limit\",                      #14\n",
    "    \"No left turn\",                      #15\n",
    "    \"No right turn\",                     #16\n",
    "    \"No overtaking\",                     #17\n",
    "    \"Maximum speed limit (90 km/h)\",     #18\n",
    "    \"Maximum speed limit (110 km/h)\",    #19\n",
    "    \"Horn prohibited\",                   #20\n",
    "    \"No parking\",                        #21\n",
    "    \"No stopping\",                       #22\n",
    "    \"Turn left\",                         #23\n",
    "    \"Turn right\",                        #24\n",
    "    \"Steep descent\",                     #25\n",
    "    \"Steep ascent\",                      #26\n",
    "    \"Narrow road\",                       #27\n",
    "    \"Narrow bridge\",                     #28\n",
    "    \"Unprotected quay\",                  #29\n",
    "    \"Road hump\",                         #30\n",
    "    \"Dip\",                               #31\n",
    "    \"Loose gravel\",                      #32\n",
    "    \"Falling rocks\",                     #33\n",
    "    \"Cattle\",                            #34\n",
    "    \"Crossroads\",                        #35\n",
    "    \"Side road junction\",                #36\n",
    "    \"Side road junction\",                #37 (again)\n",
    "    \"Oblique side road junction\",        #38\n",
    "    \"Oblique side road junction\",        #39\n",
    "    \"T-junction\",                        #40\n",
    "    \"Y-junction\",                        #41\n",
    "    \"Staggered side road junction\",      #42\n",
    "    \"Staggered side road junction\",      #43\n",
    "    \"Roundabout\",                        #44\n",
    "    \"Guarded level crossing ahead\",      #45\n",
    "    \"Unguarded level crossing ahead\",    #46\n",
    "    \"Level crossing countdown marker\",   #47\n",
    "    \"Level crossing countdown marker\",   #48\n",
    "    \"Level crossing countdown marker\",   #49\n",
    "    \"Level crossing countdown marker\",   #50\n",
    "    \"Parking\",                           #51\n",
    "    \"Bus stop\",                          #52\n",
    "    \"First aid post\",                    #53\n",
    "    \"Telephone\",                         #54\n",
    "    \"Filling station\",                   #55\n",
    "    \"Hotel\",                             #56\n",
    "    \"Restaurant\",                        #57\n",
    "    \"Refreshments\"                       #58\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 130)  \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "last_prediction = None\n",
    "stable_prediction = None\n",
    "prediction_start_time = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    \n",
    "    roi = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    roi_input = np.expand_dims(roi / 255.0, axis=0)\n",
    "\n",
    "\n",
    "    pred = model.predict(roi_input, verbose=0)\n",
    "    class_id = np.argmax(pred)\n",
    "    confidence = np.max(pred)\n",
    "\n",
    "    if confidence > 0.85:\n",
    "        predicted_class = class_labels[class_id]\n",
    "\n",
    "        \n",
    "        cv2.putText(frame, f\"{predicted_class} ({confidence*100:.1f}%)\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        \n",
    "        if predicted_class != stable_prediction:\n",
    "            stable_prediction = predicted_class\n",
    "            prediction_start_time = time.time()\n",
    "        else:\n",
    "            \n",
    "            if time.time() - prediction_start_time >= 1.0 and predicted_class != last_prediction:\n",
    "                print(f\"Detected: {predicted_class}\")\n",
    "                engine.say(f\"{predicted_class} detected\")\n",
    "                engine.runAndWait()\n",
    "                last_prediction = predicted_class\n",
    "    else:\n",
    "        stable_prediction = None\n",
    "        prediction_start_time = 0\n",
    "        cv2.putText(frame, \"Detecting...\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Traffic Sign Detection\", frame)\n",
    "\n",
    "    # Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b6b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d9e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sangavi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
